{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Israelchguevara/Deteccion-de-profesiones-en-Twitter/blob/main/Deteccion_de_profesiones_en_Twitter_GH.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eqMgbA-WYPzv"
      },
      "outputs": [],
      "source": [
        "!pip install datasets\n",
        "!pip install evaluate\n",
        "!pip install fsspec==2023.9.2\n",
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GNPI9AXnYPzx"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TxIi66meYPzy"
      },
      "outputs": [],
      "source": [
        "# Add your imports here\n",
        "import numpy as np\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "import os, random\n",
        "import torch\n",
        "\n",
        "SEED = 42\n",
        "\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed_all(SEED)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "os.environ[\"PYTHONHASHSEED\"] = str(SEED)\n",
        "\n",
        "try:\n",
        "    import torch\n",
        "    torch.manual_seed(SEED)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(SEED)\n",
        "except Exception as e:\n",
        "    print(\"Aviso: PyTorch no encontrado o sin GPU. Detalle:\", e)\n",
        "\n",
        "\n",
        "try:\n",
        "    from transformers import set_seed\n",
        "    set_seed(SEED)\n",
        "except Exception as e:\n",
        "    print(\"Aviso: Transformers no encontrado todav√≠a. Aseg√∫rate de instalarlo antes. Detalle:\", e)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jJTs4QbHYPzy"
      },
      "source": [
        "# üîç  Detecci√≥n de profesiones en tweets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LN3vYsSdYPzy"
      },
      "source": [
        "## Enunciado"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_66IvUP2YPzy"
      },
      "source": [
        "En este ejercicio vamos a trabajar con un conjunto de datos procedente de medios sociales online.\n",
        "\n",
        "Utilizaremos un subconjunto de los datos de la proyecto 1 del shared task [**ProfNER**](https://temu.bsc.es/smm4h-spanish), centrada en la detecci√≥n de menciones a profesiones en tweets publicados durante la pandemia del COVID-19. El objetivo original de la proyecto era analizar que profesiones podr√≠an haber sido especialmente vulnerables en el contexto de la crisis sanitaria.\n",
        "\n",
        "Para simplificar el ejercicio, he preparado una versi√≥n reducida del dataset original. Tu proyecto ser√° entrenar un clasificador binario basado en la arquitectura Transformers, que, dado un tweet, determine si contiene una menci√≥n expl√≠cita a una profesi√≥n (etiqueta `1`) o no (etiqueta `0`).\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p1pkJcA2YPzz"
      },
      "source": [
        "‚úÖ **Objetivos del ejercicio**\n",
        "\n",
        "A lo largo de este notebook, completar√°s las siguientes etapas para construir un clasificador de menciones a profesiones en tweets:\n",
        "\n",
        "1. **An√°lisis Exploratorio de Datos (EDA)**: Calcular estad√≠sticas b√°sicas del conjunto de datos (como el n√∫mero de ejemplos del training set, la distribuci√≥n de clases del dataset, la longitud media de los textos) o crear visualizaciones para cmprender mejor el contenido de los documentos usando wordclouds o histogramas.\n",
        "\n",
        "2. **Selecci√≥n y justificaci√≥n del modelo**: Elegir un modelo del Hub de Huggingface adecuado para los datos con los que se va a trabajar y el tipo de proyecto a desarrollar.\n",
        "\n",
        "3. **Entrenamiento del clasificador**: Entrenar el modelo de forma reproducible y evaluar su rendimiento sobreel conjunto de datos de validaci√≥n, incluyendo un classification score y matriz de confusion\n",
        "\n",
        "4. **Generaci√≥n de predicciones sobre el conjunte de test**: Aplicar el modelo entrenado al conjunto de test, y guardar las predicciones en un archivo `.tsv` de 2 columnas `id` y `label` separadas por tabulador"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DRxebHrCYPzz"
      },
      "source": [
        "# Tu resoluci√≥n (rellena las celdas marcadas)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lic-JJJ3YPzz"
      },
      "source": [
        "## Obtenci√≥n de datos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XO9FApNUYPzz"
      },
      "source": [
        "Descargamos los datos del [repositorio de Huggingface](https://huggingface.co/datasets/luisgasco/profner_classification_master)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Ku7xtfEYPzz"
      },
      "outputs": [],
      "source": [
        "#NO-MODIFY: DATA LOAD\n",
        "from datasets import load_dataset, Dataset, DatasetDict, ClassLabel\n",
        "dataset = load_dataset(\"luisgasco/profner_classification_master\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IQMIAjXbYPzz"
      },
      "source": [
        "El dataset contiene tres subsets:\n",
        "- **train** y **validation**: Contienen el identificador del tweet, el texto, y su etiqueta, que podr√° tener valor 1, si contiene una menci√≥n de una profesi√≥n; o valor 0, si no contiene una menci√≥n de una profesi√≥n.\n",
        "- **test**: El test set tambi√≠en contiene la informaci√≥n de label por un requerimiento de Huggingface, pero el contenido de esta variable es siempre \"-1\". Es decir que deber√©is predecir nuevas etiquetas una vez hay√°is entrenado el modelo utilizando el train y el validation set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r_TchxdaYPzz"
      },
      "source": [
        "## An√°lisis exploratorio de datos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hc_Ke51AYPzz"
      },
      "source": [
        "Para hacer el an√°lisis exploratorio de datos, transformamos cada subset a un pandas dataframe para mayor comodidad."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7C9VTfKEYPz0"
      },
      "outputs": [],
      "source": [
        "#NO-MODIFY: DATA LOAD\n",
        "dataset_train_df = dataset[\"train\"].to_pandas()\n",
        "dataset_val_df = dataset[\"validation\"].to_pandas()\n",
        "dataset_test_df = dataset[\"test\"].to_pandas()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yVBtNf-0YPz0"
      },
      "source": [
        "**N√∫mero de documentos**\n",
        "\n",
        "Obten con la funci√≥n `get_num_docs_evaluation()` el n√∫mero de documentos del dataset de training y validation.\n",
        "\n",
        "> Recuerda incorporar la informaci√≥n para el c√°lculo dentro del a siguiente celda, sin modificar los atributos de entrada ni de salida de la funci√≥n, ni su nombre."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OGCEHuAqYPz0"
      },
      "outputs": [],
      "source": [
        "#MODIFY: ADD INFO TO SOLVE FUNCTION\n",
        "def get_num_docs_evaluation(dataset_df):\n",
        "  # Modifica la funci√≥n.\n",
        "  num_docs = len(dataset_df)\n",
        "\n",
        "  # No modifiques el return\n",
        "  return num_docs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tDrJBoJ8YPz0"
      },
      "source": [
        "Una vez generada la funci√≥n, puedes utilizarla posteriormente para calcular resultados y comentarlos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uRyLWv9GYPz0"
      },
      "outputs": [],
      "source": [
        "# Aplica la funci√≥n\n",
        "num_docs_train = get_num_docs_evaluation(dataset_train_df)\n",
        "num_docs_val = get_num_docs_evaluation(dataset_val_df)\n",
        "\n",
        "print(f\"Number of documents in training set: {num_docs_train}\")\n",
        "print(f\"Number of documents in validation set: {num_docs_val}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gc7Yvs5nYPz0"
      },
      "source": [
        "**N√∫mero de documentos duplicados**\n",
        "\n",
        "Obten con la funci√≥n `detect_duplicates_evaluation()` el n√∫mero de documentos duplicados del dataset de training y validation.\n",
        "\n",
        "> Recuerda incorporar la informaci√≥n para el c√°lculo dentro del a siguiente celda, sin modificar los atributos de entrada ni de salida de la funci√≥n, ni su nombre."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ye2RE3zjYPz0"
      },
      "outputs": [],
      "source": [
        "#MODIFY: ADD INFO TO SOLVE FUNCTION\n",
        "def detect_duplicates_evaluation(dataset_df):\n",
        "  # Modifica la funci√≥n.\n",
        "  num_duplicates = dataset_df.duplicated().sum()\n",
        "\n",
        "  # No modifiques el return\n",
        "  return num_duplicates"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z4QGfHOsYPz0"
      },
      "source": [
        "Una vez generada la funci√≥n, puedes utilizarla posteriormente para calcular resultados y comentarlos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uUvu4yVyYPz0"
      },
      "outputs": [],
      "source": [
        "# Aplica la funci√≥n\n",
        "num_duplicates_train = detect_duplicates_evaluation(dataset_train_df)\n",
        "num_duplicates_val = detect_duplicates_evaluation(dataset_val_df)\n",
        "\n",
        "print(f\"Number of duplicate documents in training set: {num_duplicates_train}\")\n",
        "print(f\"Number of duplicate documents in validation set: {num_duplicates_val}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uZCXmc2vYPz0"
      },
      "source": [
        "**N√∫mero de documentos por cada clase:**\n",
        "\n",
        "\n",
        "Obten con la funci√≥n `analyse_num_labels_evaluation()` para calcular el n√∫mero de documentos de cada categor√≠a en el dataset\n",
        "\n",
        "> Recuerda incorporar la informaci√≥n para el c√°lculo dentro del a siguiente celda, sin modificar los atributos de entrada ni de salida de la funci√≥n, ni su nombre."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gd80QZMzYPz0"
      },
      "outputs": [],
      "source": [
        "#MODIFY: ADD INFO TO SOLVE FUNCTION\n",
        "def analyse_num_labels_evaluation(dataset_df):\n",
        "  # Modifica la funci√≥n.\n",
        "  num_positives = dataset_df[dataset_df['label'] == 1].shape[0]\n",
        "  num_negatives = dataset_df[dataset_df['label'] == 0].shape[0]\n",
        "\n",
        "  # No modifiques el return\n",
        "  return num_positives, num_negatives"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ZvZyqSuYPz0"
      },
      "source": [
        "Una vez generada la funci√≥n, puedes utilizarla posteriormente para calcular resultados y comentarlos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gz-5hA08YPz0"
      },
      "outputs": [],
      "source": [
        "# Aplica la funci√≥n\n",
        "num_positives_train, num_negatives_train = analyse_num_labels_evaluation(dataset_train_df)\n",
        "num_positives_val, num_negatives_val = analyse_num_labels_evaluation(dataset_val_df)\n",
        "\n",
        "print(f\"Training set: Positive examples: {num_positives_train}, Negative examples: {num_negatives_train}\")\n",
        "print(f\"Validation set: Positive examples: {num_positives_val}, Negative examples: {num_negatives_val}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q1LI8RLYYPz0"
      },
      "source": [
        "**Distribuci√≥n de la longitud de los tweet en caracteres:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Omb8JOigYPz0"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Calculate the length of each tweet\n",
        "dataset_train_df['tweet_length'] = dataset_train_df['text'].apply(len)\n",
        "dataset_val_df['tweet_length'] = dataset_val_df['text'].apply(len)\n",
        "\n",
        "# Plot the distribution of tweet lengths\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.hist(dataset_train_df['tweet_length'], bins=50, alpha=0.7, label='Train')\n",
        "plt.hist(dataset_val_df['tweet_length'], bins=50, alpha=0.7, label='Validation')\n",
        "plt.xlabel('Tweet Length (characters)')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('Distribution of Tweet Lengths')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ei80iqv8YPz0"
      },
      "source": [
        "**Interpretaci√≥n del gr√°fico de Distribuci√≥n de la Longitud de los Tweets:**\n",
        "\n",
        "El histograma visualiza c√≥mo se distribuye la longitud (en caracteres) de los tweets en tus conjuntos de datos de entrenamiento y validaci√≥n.\n",
        "\n",
        "Al observar este histograma, puedes entender caracter√≠sticas clave de tus datos, como:\n",
        "\n",
        "*   **La longitud m√°s com√∫n:** La barra m√°s alta indica el rango de longitud donde se concentra la mayor cantidad de tweets.\n",
        "*   **La dispersi√≥n de las longitudes:** Puedes ver si las longitudes est√°n muy agrupadas o si hay una amplia variaci√≥n.\n",
        "*   **Presencia de tweets muy cortos o muy largos:** Los extremos del histograma te mostrar√°n si hay una cantidad significativa de tweets con longitudes inusuales.\n",
        "*   **Comparaci√≥n entre conjuntos:** Las barras para el conjunto de entrenamiento (azul) y validaci√≥n (naranja) te permiten ver si la distribuci√≥n de longitudes es similar en ambos conjuntos.\n",
        "\n",
        "Este an√°lisis es √∫til para tareas de procesamiento de texto, ya que puede guiar decisiones sobre la longitud m√°xima a considerar para la tokenizaci√≥n o el padding."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9a6GANGQYPz0"
      },
      "source": [
        "**An√°lisis de contenido de los tweets**\n",
        "\n",
        "Para ello utiliza wordclouds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MX7LpMsVYPz0"
      },
      "outputs": [],
      "source": [
        "!pip install wordcloud\n",
        "\n",
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Combine text from positive and negative examples\n",
        "positive_text = \" \".join(dataset_train_df[dataset_train_df['label'] == 1]['text'])\n",
        "negative_text = \" \".join(dataset_train_df[dataset_train_df['label'] == 0]['text'])\n",
        "\n",
        "# Create wordcloud for positive examples\n",
        "wordcloud_positive = WordCloud(width=800, height=400, background_color='white').generate(positive_text)\n",
        "\n",
        "# Create wordcloud for negative examples\n",
        "wordcloud_negative = WordCloud(width=800, height=400, background_color='white').generate(negative_text)\n",
        "\n",
        "# Display the wordclouds\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.imshow(wordcloud_positive, interpolation='bilinear')\n",
        "plt.axis('off')\n",
        "plt.title('Wordcloud for Positive Examples (Profession Mentioned)')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.imshow(wordcloud_negative, interpolation='bilinear')\n",
        "plt.axis('off')\n",
        "plt.title('Wordcloud for Negative Examples (No Profession Mentioned)')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hk9-WxAQYPz1"
      },
      "source": [
        "**Interpretaci√≥n de Wordclouds:**\n",
        "\n",
        "*   **\"Wordcloud for Positive Examples (Profession Mentioned)\"**: Esta wordcloud visualiza las palabras que aparecen con mayor frecuencia en los tweets del conjunto de entrenamiento que han sido etiquetados con la clase `1`. La etiqueta `1` en este conjunto de datos indica que el tweet **contiene una menci√≥n expl√≠cita a una profesi√≥n**. Por lo tanto, esta wordcloud te muestra las palabras m√°s comunes en los tweets donde se habla de profesiones.\n",
        "\n",
        "*   **\"Wordcloud for Negative Examples (No Profession Mentioned)\"**: Esta wordcloud visualiza las palabras que aparecen con mayor frecuencia en los tweets del conjunto de entrenamiento que han sido etiquetados con la clase `0`. La etiqueta `0` en este conjunto de datos indica que el tweet **no contiene una menci√≥n expl√≠cita a una profesi√≥n**. Esta wordcloud, por lo tanto, te muestra las palabras m√°s comunes en los tweets que no hablan de profesiones.\n",
        "\n",
        "Comparar estas dos wordclouds te permite identificar visualmente las palabras que son m√°s distintivas o frecuentes en cada una de las dos categor√≠as de tweets, lo cual es un paso √∫til en el an√°lisis exploratorio para entender las diferencias entre las clases."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c9iaJCPIYPz1"
      },
      "source": [
        "## Tokenizaci√≥n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DFFv3OKxYPz1"
      },
      "source": [
        "El texto del dataset no est√° preparado para ser introducido en un modelo Transformers. Lleva a cabo el proceso de tokenizaci√≥n."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a1-LSv7wYPz1"
      },
      "outputs": [],
      "source": [
        "# IMPORTS\n",
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
        "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wiwScsGGYPz1"
      },
      "source": [
        "Selecciona un modelo apropiado para la proyecto:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CiqGPbnPYPz1"
      },
      "source": [
        "> Recuerda que en la siguiente celda s√≥lo debes asignar un valor a model_name. No a√±adas m√°s informaci√≥n en la celda."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cTcaAXiEYPz1"
      },
      "outputs": [],
      "source": [
        "#NO-MODIFY: VARIABLE NAME\n",
        "# BERT-based model trained on Spanish data\n",
        "model_name = 'dccuchile/bert-base-spanish-wwm-cased'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ei_FWxYrYPz4"
      },
      "source": [
        "## Justificaci√≥n del modelo\n",
        "\n",
        "Para este ejercicio se emplea **BETO** (`dccuchile/bert-base-spanish-wwm-cased`), un BERT preentrenado **espec√≠fico para espa√±ol** y con **Whole Word Masking (WWM)**. Las razones de esta elecci√≥n son:\n",
        "\n",
        "- **Adecuaci√≥n al idioma**: el corpus de preentrenamiento es espa√±ol, lo que mejora la cobertura l√©xica, morfol√≥gica y fen√≥menos propios del idioma frente a modelos multiling√ºes.\n",
        "- **WWM**: enmascara palabras completas durante el preentrenamiento, beneficiando tareas de clasificaci√≥n de texto donde la integridad de las unidades l√©xicas aporta se√±al.\n",
        "- **Balance capacidad/recursos**: tama√±o base (~110M par√°metros), entrenable en una GPU de docencia con tiempos razonables.\n",
        "- **Resultados contrastados**: BETO ha mostrado buen rendimiento en m√∫ltiples tareas de PLN en espa√±ol (clasificaci√≥n, NER, an√°lisis de sentimiento), por lo que es una opci√≥n s√≥lida como baseline fuerte.\n",
        "\n",
        "Alternativas razonables ser√≠an **mBERT** o modelos recientes como **roberta-base-bne** (es-core news) o **ALBERT/DeBERTa** adaptados al espa√±ol, pero BETO ofrece un equilibrio muy adecuado entre rendimiento y coste computacional para este entorno docente.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6wwnMYqVYPz4"
      },
      "source": [
        "Puedes continuar con el proceso aqu√≠:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2WD2PI9dYPz4"
      },
      "outputs": [],
      "source": [
        "# Load the tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Tokenize the datasets\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
        "\n",
        "tokenized_datasets = dataset.map(tokenize_function, batched=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ewz7lxywYPz4"
      },
      "source": [
        "## Fine-tuning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "788vxMGVYPz4"
      },
      "source": [
        "Carga el model para ser ajustado posteriormente:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XBX5t-mJYPz4"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForSequenceClassification\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7AISicduYPz4"
      },
      "source": [
        "### Configuracion training_args"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lpBtkVI9YPz4"
      },
      "source": [
        "Configura los par√°metros de entrenamiento del modelo.\n",
        "\n",
        "\n",
        ">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dbb9C4BnYPz4"
      },
      "source": [
        "> Recuerda que en la siguiente celda s√≥lo debes asignar atributos a la variable training_args. No a√±adas  otras variables en la celda"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z5eN6eIrYPz4"
      },
      "outputs": [],
      "source": [
        "from transformers import TrainingArguments\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    seed=SEED,\n",
        "    output_dir=\"./results\",  # Directorio de salida para los resultados del entrenamiento\n",
        "    learning_rate=2e-5,  # Tasa de aprendizaje\n",
        "    per_device_train_batch_size=32,  # Tama√±o del batch por dispositivo para entrenamiento\n",
        "    per_device_eval_batch_size=32,  # Tama√±o del batch por dispositivo para evaluaci√≥n\n",
        "    num_train_epochs=3,  # N√∫mero de √©pocas de entrenamiento\n",
        "    weight_decay=0.01,  # Decaimiento del peso\n",
        "    do_train=False,\n",
        "    do_eval=True,\n",
        "    logging_dir=\"./logs\",\n",
        "    report_to=\"none\"\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    eval_dataset=tokenized_datasets[\"validation\"],\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B5R0AgsaYPz5"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import evaluate\n",
        "\n",
        "metric = evaluate.load(\"f1\")\n",
        "\n",
        "import evaluate, numpy as np\n",
        "precision_metric = evaluate.load(\"precision\")\n",
        "recall_metric = evaluate.load(\"recall\")\n",
        "f1_metric = evaluate.load(\"f1\")\n",
        "accuracy_metric = evaluate.load(\"accuracy\")\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    preds = np.argmax(logits, axis=-1)\n",
        "    return {\n",
        "        \"precision\": precision_metric.compute(predictions=preds, references=labels, average=\"binary\")[\"precision\"],\n",
        "        \"recall\":    recall_metric.compute(predictions=preds, references=labels, average=\"binary\")[\"recall\"],\n",
        "        \"f1\":        f1_metric.compute(predictions=preds, references=labels, average=\"binary\")[\"f1\"],\n",
        "        \"accuracy\":  accuracy_metric.compute(predictions=preds, references=labels)[\"accuracy\"],\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tsv04NAJYPz5"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RirKRj4PYPz5"
      },
      "source": [
        "### Ajuste del modelo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mX667e08YPz5"
      },
      "source": [
        "Lleva a cabo el ajuste del modelo:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZoeZnKTfYPz5"
      },
      "outputs": [],
      "source": [
        "from transformers import Trainer\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_datasets[\"train\"],\n",
        "    eval_dataset=tokenized_datasets[\"validation\"],\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QBKYBnVfYPz5"
      },
      "outputs": [],
      "source": [
        "trainer.evaluate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FzTEfd9CYPz5"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Assuming y_true and y_pred are already available from a previous prediction/evaluation step\n",
        "# If not, you would need to run trainer.predict() first to get them.\n",
        "# Based on the notebook state, y_true and y_pred were generated in cell ZUkg3iH7Uf7m\n",
        "\n",
        "# Generate classification report\n",
        "report = classification_report(y_true, y_pred, target_names=[\"No Profession\", \"Profession\"]) # Assuming class names based on previous analysis\n",
        "\n",
        "print(\"Classification Report for Validation Set:\")\n",
        "print(report)\n",
        "\n",
        "print(\"\\n\" + \"=\"*50 + \"\\n\") # Separator for clarity\n",
        "\n",
        "# Generate and display confusion matrix (Code moved from cell ZUkg3iH7Uf7m)\n",
        "cm = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "# Define class labels for display\n",
        "class_names = [\"No Profession\", \"Profession\"] # Assuming 0 is No Profession and 1 is Profession based on previous analysis\n",
        "\n",
        "# Display the confusion matrix\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_names)\n",
        "disp.plot(cmap=plt.cm.Blues)\n",
        "plt.title(\"Confusion Matrix for Validation Set\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7LD_RMh_YPz5"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HsVxzcDZYPz5"
      },
      "source": [
        "## Genera predicciones"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Po09RfbtYPz5"
      },
      "source": [
        "Genera predicciones sobre el test set. Recuerda que el archivo que generes y adjuntes al ejercicio debe tener dos columnas:\n",
        "\n",
        "\n",
        "| id         | label |\n",
        "|------------|-------|\n",
        "| 1234567890 | 1     |\n",
        "| 1234567891 | 0     |\n",
        "| 1234567892 | 0     |\n",
        "| 1234567893 | 1     |\n",
        "\n",
        "- El archivo debe estar en formato **TSV** (separado por tabuladores).\n",
        "- Debe contener exactamente **dos columnas**: `id` y `label`.\n",
        "- Es obligatorio incluir la **cabecera**.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HvomONxQYPz5"
      },
      "outputs": [],
      "source": [
        "tokenized_datasets[\"test\"][\"label\"][0:4]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5zJ_iOi4YPz5"
      },
      "outputs": [],
      "source": [
        "def fix_labels(example):\n",
        "    example[\"label\"] = 1  # O lo que toque\n",
        "    return example\n",
        "ClassLabel\n",
        "# Aplica la funci√≥n al dataset de evaluaci√≥n\n",
        "dataset_test = tokenized_datasets[\"test\"].map(fix_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "InNBwHCSYPz5"
      },
      "outputs": [],
      "source": [
        "label2id = {\"SIN_PROFESION\":0, \"CON_PROFESION\":1}\n",
        "\n",
        "#NO-MODIFY: DATA LOAD\n",
        "from datasets import load_dataset, Dataset, DatasetDict, ClassLabel\n",
        "dataset = load_dataset(\"luisgasco/profner_classification_master\")\n",
        "\n",
        "dataset_test = dataset[\"test\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z70bKeW4YPz5"
      },
      "outputs": [],
      "source": [
        "x_id = [x[\"tweet_id\"] for x in dataset_test]\n",
        "documentos = [x[\"text\"] for x in dataset_test]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0dQpwwDwYPz5"
      },
      "outputs": [],
      "source": [
        "documentos[0:3]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oM6OkdN6YPz5"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "import torch\n",
        "batch_size = 32\n",
        "preds = []\n",
        "\n",
        "for i in tqdm(range(0, len(documentos), batch_size)):\n",
        "    batch = documentos[i:i+batch_size]\n",
        "    # Tokenizar el batch\n",
        "    tokenized_batch = tokenizer(batch, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
        "    with torch.no_grad():\n",
        "        # Mover el batch tokenizado al mismo dispositivo que el modelo\n",
        "        tokenized_batch = {k: v.to(model.device) for k, v in tokenized_batch.items()}\n",
        "        batch_preds = model(**tokenized_batch)\n",
        "    preds.extend(batch_preds.logits)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YWcHHYHaYPz5"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Get the predicted labels from the logits\n",
        "predicted_labels = [np.argmax(p.cpu().numpy()) for p in preds]\n",
        "\n",
        "# Create the num_preds list\n",
        "num_preds = predicted_labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z2QwPAFGYPz5"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "output_df2 = pd.DataFrame(\n",
        "    {'id': x_id,\n",
        "     'label': num_preds\n",
        "    })"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "03n5c3uaYPz5"
      },
      "outputs": [],
      "source": [
        "output_df2.head(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BFCSgVCVYPz5"
      },
      "outputs": [],
      "source": [
        "output_df2.to_csv(\"predicciones.tsv\", sep=\"\\t\",index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XhJUFEx6YPz6"
      },
      "outputs": [],
      "source": [
        "output_df2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xmp8J-4UYPz6"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "files.download(\"predicciones.tsv\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}