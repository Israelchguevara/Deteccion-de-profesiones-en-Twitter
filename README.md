# Deteccion-de-profesiones-en-Twitter
Proyecto de Text Mining / NLP para clasificar tweets menciona al menos una profesiÃ³n (label=1) o ninguna (label=0). El flujo cubre EDA, preprocesado, fine-tuning de un modelo Transformers (Hugging Face) y generaciÃ³n de predicciones sobre un conjunto de test
DetecciÃ³n de profesiones en Twitter â€” Proyecto de NLP

El flujo incluye EDA, preprocesado, fine-tuning de un modelo Transformers, evaluaciÃ³n con mÃ©tricas robustas y exportaciÃ³n de predicciones en formato .tsv.

ğŸ¯ Objetivos

Construir un pipeline reproducible de clasificaciÃ³n binaria de texto.

Comparar un baseline clÃ¡sico con un modelo Transformers y seleccionar el mejor.

Generar un archivo de predicciones estandarizado para uso/evaluaciÃ³n externa.

ğŸ—‚ï¸ Estructura del repositorio
â”œâ”€ notebooks/
â”‚  â””â”€ Deteccion_de_profesiones_en_Twitter_GH.ipynb   # cuaderno principal (limpio para GitHub)
â”œâ”€ src/
â”‚  â”œâ”€ eda.py               # anÃ¡lisis exploratorio (opcional)
â”‚  â”œâ”€ preprocess.py        # limpieza, normalizaciÃ³n, tokenizaciÃ³n
â”‚  â”œâ”€ train.py             # entrenamiento + validaciÃ³n
â”‚  â””â”€ infer.py             # inferencia + exportaciÃ³n a TSV
â”œâ”€ data/
â”‚  â”œâ”€ raw/                 # datos en bruto (no versionados)
â”‚  â””â”€ processed/           # datos limpios/features (no versionados)
â”œâ”€ outputs/
â”‚  â””â”€ predicciones.tsv     # archivo de predicciones (no versionado)
â”œâ”€ figures/                # grÃ¡ficos (no versionados)
â”œâ”€ .gitignore
â””â”€ README.md


El repo ignora data/, models/, outputs/ y figures/ para evitar subir datos/artefactos pesados.

ğŸ”§ Entorno y dependencias (sugeridas)

Python 3.10+

pandas, numpy, scikit-learn

transformers, datasets, torch (o tensorflow)

matplotlib, seaborn (visualizaciÃ³n)

wordcloud (opcional para EDA)

InstalaciÃ³n rÃ¡pida:

python -m venv .venv
source .venv/bin/activate     # Windows: .venv\Scripts\activate
pip install -U pip
pip install pandas numpy scikit-learn transformers datasets torch matplotlib seaborn wordcloud

â–¶ï¸ Uso rÃ¡pido
1) Datos

Coloca los ficheros en data/raw/ con, al menos:

train.csv â†’ columnas: id, text, label

valid.csv â†’ columnas: id, text, label

test.csv â†’ columnas: id, text (sin label)

Si tu dataset estÃ¡ en otro idioma/estructura, ajusta src/preprocess.py.

2) Entrenamiento

Ejemplo con un modelo multilingÃ¼e:

python -m src.train \
  --train data/raw/train.csv \
  --valid data/raw/valid.csv \
  --model_name "bert-base-multilingual-cased" \
  --max_length 128 \
  --batch_size 32 \
  --epochs 3 \
  --lr 2e-5 \
  --out_dir models/best

3) Inferencia y exportaciÃ³n
python -m src.infer \
  --model_dir models/best \
  --test data/raw/test.csv \
  --out outputs/predicciones.tsv


Formato de outputs/predicciones.tsv:

id	label
123	1
456	0
...


Separador: tabulador

Cabeceras: id, label

Sin Ã­ndice adicional

4) Notebook

TambiÃ©n puedes ejecutar el flujo completo desde:

notebooks/Deteccion_de_profesiones_en_Twitter_GH.ipynb


El cuaderno estÃ¡ limpio (sin outputs ni metadatos ruidosos) y documentado paso a paso.

ğŸ“Š MÃ©tricas y validaciÃ³n

MÃ©trica principal sugerida: F1-score (Ãºtil con desbalance).

Reporte complementario: Accuracy, Precision, Recall, Matriz de confusiÃ³n.

ValidaciÃ³n con hold-out (train/valid) o K-Fold estratificado.

Semillas fijas para reproducibilidad.

ğŸ§  Modelado (resumen)

Baseline: LogisticRegression / LinearSVC con TF-IDF.

Transformers: fine-tuning (BERT/RoBERTa multilingÃ¼e) con tokenizaciÃ³n, max_length acorde al tweet y early stopping opcional.

Explicabilidad (opcional): inspecciÃ³n de ejemplos frontera, curvas PR/ROC y anÃ¡lisis de errores.

â™»ï¸ Buenas prÃ¡cticas

Pipelines (scikit-learn / transformers) con parÃ¡metros versionados.

Control de data leakage y consistencia de splits.

No versionar datos/artefactos pesados (ya cubierto por .gitignore).

Documentar supuestos del dataset (lengua, hashtags, menciones, emojis).

ğŸš€ Roadmap

AÃ±adir tuning con optuna/GridSearch.

Aumentar robustez con data augmentation para textos cortos.

Publicar un Space (Hugging Face) o endpoint de inferencia liviano.
